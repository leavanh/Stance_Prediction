\chapter{Discussion, Limitations and Outlook}\label{discussion}

\section{Discussion}

The study examined the effectiveness of incorporating information from political party manifestos to enhance the accuracy of models that predict party stances. The models proposed in this study employ user-generated statements and relevant passages from party manifestos as inputs to predict the political party most likely to agree with the statement.

The study's findings indicate that while incorporating data from political party manifestos could in some cases improve the model's ability to predict party stances, the improvement was not substantial. The performance of various semantic search techniques differed, with some techniques (IS-BERT) providing better results than others, but no technique showed great improvement. The performance of the model is not seriously affected by the various input patterns. In addition, the study compared the performance of two NLP models, ELECTRA \citep{clark2020electra} and BERT \citep{devlin2018bert}, and found that ELECTRA was better suited for this task. Overall, it can be said that providing context in the form of party manifesto passages doesn't make the model much better.

In an effort to make the context more understandable, it was summarized. But this resulted in a worse performance when compared to the models without summarization. The context itself may be too confusing or complex to be adequately summarized, or the IGEL model \citep{igel} used for summarization may simply not be the best model for this task.

The lack of noticeable improvement of a model with vs without context could have a number of causes. Firstly, it is possible that the models are unable to effectively handle the increased complexity introduced by the addition of information from party manifestos, leading to confusion and reduced accuracy. Second, party manifestos could include an excessive amount of ambiguous or meaningless language that doesn't add any useful information to the model and could even hinder its performance by adding extra noise. Finally, it is plausible that using semantic search methods to extract relevant sentences from party manifestos yields subpar results, resulting in less-than-ideal inputs that further perplex the model. These potential explanations warrant further investigation that could help shed light on the limited effectiveness.

\section{Limitations}

There are some general restrictions in addition to these three possible causes for the poor improvement of the model through context adding. This study's language focus is one major drawback. It only looked at how well German-language political party manifestos could be used to integrate context. The results might not apply to other languages as a result. The model may be impacted by the distinctive structural features, idiomatic expressions, and cultural contexts of the language. Therefore, more research is required to determine whether results differ between languages.

Secondly, the study's scope was restricted to a single setting, namely political stance prediction. Although this is a relevant and important application of NLP, the effectiveness of the suggested idea may differ depending on the task or application. Additional context may be very beneficial to a model and improve performance in a different setting than this one.

This study's third drawback is that it only used party manifestos as its context. Manifestos are a great source of information about political parties' ideologies, but they might not include all the necessary details and may be (intentionally) vague and general. The predictive power of the model may be improved by using data from additional sources, such as speeches, interviews, and news articles. Future studies might examine how well various context types can be incorporated into the model. By including these extra sources of information in the models, one may be able to create more reliable and beneficial models that can better assist people in making informed decisions. It is important to keep in mind that incorporating these additional sources will also present new challenges, including the need to eliminate noise and irrelevant data as well as different input patterns, and that additional research will be required to help solve these problems.

\section{Outlook}

The semantic search techniques and patterns employed were limited. Although the study used a variety of semantic search strategies and input patterns, there are still a wide range of strategies and patterns that have the potential to enhance the performance of the model. Additionally, it would seem that better semantic search methods are needed, particularly for German. Investigating the use of other semantic search models, such as a Cohere model \citep{cohere} created specifically for German semantic search, may be worthwhile.

It might also be interesting to look into how the number of similar sentences used as context affects the results and whether or not using only sentences that are more similar than a given threshold could improve performance.

Furthermore, it might be advantageous to further investigate summarization in order to give the model information that is more condensed and understandable. While one experiment was conducted and did not yield promising results, there are many more models that can be tested. Trying different commands on those models is certainly useful as well, since they greatly impact the outcomes. The small-scale experiment conducted here shouldn't discourage anyone from pursuing this avenue further.

Another approach to the problem of including party manifestos in a predictive model is to change the emphasis from sentences to paragraphs. It's possible that the relevant info is spread out over several sentences, forming a coherent paragraph, rather than being contained in a single sentence. The most relevant paragraph can be found using a semantic search, which can then be summarized to produce a concise sentence that conveys the main idea. This approach could potentially address the issue of party manifestos being overly complex and lacking in clarity. Further research is needed to determine the effectiveness of this approach and how it compares to sentence-level analysis.

In order to identify input patterns that are more effective at predicting party stances, it may also be useful to experiment with even more of them. Small adjustments to the current patterns in addition to completely different patterns may have an impact on the performance. For instance, it might be beneficial to remove or swap out the square brackets around the context for something else.

The same is true for models. Although the study used the well-known NLP models ELECTRA \citep{clark2020electra} and BERT \citep{devlin2018bert}, there are more recent and superior models available, such as, for example, T5 \citep{xue2020mt5}. It would be beneficial to test out various models and hyperparameters to see which ones work best for this task, with a focus on finding models that are suited for the German language. However, these models may be computationally expensive, not easily fine-tunable, or not available open source. These are limitations that have to be considered.

Generally speaking, before a model used for predicting political stances can be put to use, its performance must first improve from the results of these experiments. It is crucial to consider the potential implications of using the proposed model in political decision-making. Given the sensitivity of politics, it is essential that the model is capable of accurately predicting political stances and is reliable enough to handle unforeseen scenarios without generating inaccurate results. If the model were to be utilized as a decision aid for voting, the consequences of its errors could be severe. By predicting the wrong parties, the algorithm would be able to change votes. Therefore, it is also imperative to investigate any potential biases in the model to ensure its reliability and accuracy (as done with ChatGPT in \citet{hartmann2023political} for example).

It's also important to keep in mind that party politics can change over time and can differ in different states or regions. The suggested models face a challenge because, in order to remain accurate, they must be able to adjust to shifting political dynamics. It might be necessary to continually update the model with the most recent political data and train it in various political contexts in order to address this challenge. Incorporating feedback mechanisms that enable users to report any errors or discrepancies in the model's predictions may also be beneficial. Doing so could help the model perform better over time.

To improve model performance, the thesis proposes adding context. However, since new models are constantly being released, things change. Models like ChatGPT \citep{chatgpt} might not need additional context. Therefore, it may be worthwhile to consider the possibility of completely discarding the notion of semantic search and input patterns and instead looking into whether models like ChatGPT \citep{chatgpt} outperform fine-tuned models on tasks like political stance prediction. This would require a shift in research focus towards understanding how to obtain the best results from such models and ensuring that these results are reliable and unbiased.